{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten\n",
    "\n",
    "Funktionen:\n",
    "- def process_files:  zum Verarbeiten aller Files\n",
    "- def test_stationarity(data, variable_name): zum Testen der Stationarität eines dataframes -> Ausgabe: Liste mit Tupeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten importieren und mergen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Daten zusammenfügen\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import lagrange\n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from hampel import hampel\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import skewtest\n",
    "\n",
    "\n",
    "# Verzeichnis mit den Dateien angeben\n",
    "csv_files_all = \"C:/Users/Lotta/OneDrive/Uni/Master/Masterarbeit Python 3.12/Daten/Daten Forward Imputation/*.csv\"\n",
    "dateien = glob.glob(csv_files_all)\n",
    "    \n",
    "\n",
    "# Initialisierung der Listen für die verschiedenen Dateitypen\n",
    "daily_data, hourly_data, weekly_data, monthly_data = [], [], [], []\n",
    "dfs_hourly, dfs_daily, dfs_weekly, dfs_monthly = [], [], [], []\n",
    "\n",
    "#Betrachtungszeitraum festlegen\n",
    "start_date = '2020-12-25' #Daten des Vorjahrs für die Imputation\n",
    "end_date = '2022-02-01'\n",
    "\n",
    "# Funktion zum Verarbeiten der Dateien\n",
    "def process_files(file_list, date_type, dfs_list):\n",
    "    for file in file_list:\n",
    "        try:\n",
    "            # Lese jede CSV-Datei ein\n",
    "            df = pd.read_csv(file, encoding='ISO-8859-1', sep=';', header=0, dtype={'column': str}, low_memory=False)  # Passe Encoding und Separator an\n",
    "            df.columns = df.columns.str.replace('ï»¿', '', regex=True)  # Entferne 'ï»¿' beim Datum\n",
    "            # Überprüfe, ob eine Spalte 'Date' existiert\n",
    "            if 'Date' in df.columns:\n",
    "                # Versuche die 'Date'-Spalte in datetime zu konvertieren\n",
    "                df['Date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True)\n",
    "                # Filtere die Daten basierend auf dem Betrachtungszeitraum\n",
    "                df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "                # Lösche alle Spalten, die mit 'Unnamed' anfangen\n",
    "                df = df.loc[:, ~df.columns.str.startswith('Unnamed')]\n",
    "                dfs_list.append(df)\n",
    "            else:\n",
    "                print(f\"Die Datei {file} enthält keine 'Date'-Spalte und wird übersprungen.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Verarbeiten der Datei {file}: {e}\")\n",
    "\n",
    "    # Kombiniere alle DataFrames basierend auf der 'Date'-Spalte\n",
    "    if dfs_list:\n",
    "        merged_df = pd.concat(dfs_list, ignore_index=True).sort_values(by='Date')\n",
    "        merged_df = merged_df.groupby('Date', as_index=False).first()\n",
    "        return merged_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Zuordnung der Dateien basierend auf ihrem Namen\n",
    "for file in dateien:\n",
    "    if 'daily' in os.path.basename(file):\n",
    "        daily_data.append(file)\n",
    "    elif 'hourly' in os.path.basename(file):\n",
    "        hourly_data.append(file)\n",
    "    elif 'weekly' in os.path.basename(file):\n",
    "        weekly_data.append(file)\n",
    "    elif 'monthly' in os.path.basename(file):\n",
    "        monthly_data.append(file)\n",
    "\n",
    "# Verarbeite jede Datengruppe und kombiniere die DataFrames\n",
    "# Verarbeite jede Datengruppe mit Train/Test-Split\n",
    "df_hourly = process_files(hourly_data, 'hourly', dfs_hourly)\n",
    "df_daily = process_files(daily_data, 'daily', dfs_daily)\n",
    "df_weekly = process_files(weekly_data, 'weekly', dfs_weekly)\n",
    "df_monthly = process_files(monthly_data, 'monthly', dfs_monthly)\n",
    "\n",
    "# Zusammenfassen der Ergebnisse\n",
    "merged_dfs = [df_hourly, df_daily, df_weekly, df_monthly]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing für imputed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Upsampling\n",
    "\n",
    "\n",
    "#Interpolieren der Daten\n",
    "for df in merged_dfs: \n",
    "    if 'Date' in df.columns:\n",
    "        df.set_index('Date', inplace=True) # Setze die 'Date'-Spalte als Index\n",
    "    else:\n",
    "        print(f\"'Date' column not found in dataframe with columns: {df.columns}\")\n",
    "\n",
    "# Resample auf tägliche Daten und Interpolation der numerischen Spalten\n",
    "daily_df_from_weekly = df_weekly.resample('D').interpolate(method= 'cubic')\n",
    "daily_from_hourly = df_hourly.resample ('D').median()\n",
    "\n",
    " \n",
    "# Resample die Daten auf täglich und führe eine Interpolation mit cubic durch\n",
    "for  col in df_monthly:\n",
    "    if col.startswith('A'):\n",
    "        df_monthly[col] = df_monthly[col].div(df_monthly.index.days_in_month, axis=0).round(3)\n",
    "        \n",
    "daily_from_monthly = df_monthly.resample('D').interpolate(method='cubic')   \n",
    "# Setze alle Werte bis zum Startdatum der Interpolation auf 0 im daily_from_monthly DataFrame\n",
    "all_daily_dfs = [daily_df_from_weekly, daily_from_hourly, daily_from_monthly, df_daily]\n",
    "\n",
    "\n",
    "#combine all daily data frames\n",
    "fi_final_df = pd.concat(all_daily_dfs, axis=1)\n",
    "fi_final_df.index = pd.to_datetime(fi_final_df.index)\n",
    "\n",
    "fi_final_df = fi_final_df.loc[:, ~fi_final_df.columns.str.startswith('Unnamed')]\n",
    "#Lag von 1 Tag einbaune, da gestrige Daten die neusten Daten sind, die vorliegen\n",
    "columns_to_lag_one = [col for col in fi_final_df.columns if col != 'S-Gas DA NL']\n",
    "\n",
    "# Daten um einen Tag verschieben (Lag)\n",
    "fi_final_df[columns_to_lag_one] = fi_final_df[columns_to_lag_one].shift(1)\n",
    "\n",
    "\n",
    "# fi_final_df auf drei Nachkommastellen runden\n",
    "fi_final_df = fi_final_df.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Up- und downgesampelte Daten plotten\n",
    "Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#alle monatlichen in getrennten plots plotten\\nnum_columns = len(daily_from_monthly.columns)  # Anzahl der Spalten\\nfig, axes = plt.subplots(num_columns, 1, figsize=(15, 6 * num_columns))\\n\\n# Wenn es nur eine Spalte gibt, sorgt dieser Code dafür, dass axes eine einzelne Achse ist\\nif num_columns == 1:\\n    axes = [axes]\\n\\n# Gehe jede Spalte durch und erstelle ein Diagramm\\nfor i, column in enumerate(daily_from_monthly.columns):\\n    # Daily Interpolated plot\\n    axes[i].plot(daily_from_monthly.index, daily_from_monthly[column], label=f\\'Daily Interpolated {column}\\', linestyle=\\'-\\', marker=\\'\\')\\n\\n    # Monthly Data plot\\n    axes[i].plot(merged_df_monthly.index, merged_df_monthly[column], label=f\\'Monthly Data {column}\\', linestyle=\\'--\\', marker=\\'o\\')\\n\\n    # Achsenbeschriftungen und Titel\\n    axes[i].set_xlabel(\"Datum\")\\n    axes[i].set_ylabel(\"Wert\")\\n    axes[i].set_title(f\"Vergleich: Daily Interpolated vs. Monthly Data ({column})\")\\n    axes[i].legend()\\n    axes[i].grid(True)\\n\\n# Layout anpassen, um Überschneidungen zu vermeiden\\nplt.tight_layout()\\nplt.show()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Diagramm für wöchentliche Daten\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot für jede numerische Spalte in daily_df_from_weekly\n",
    "for column in daily_df_from_weekly.columns:\n",
    "    plt.plot(daily_df_from_weekly.index, daily_df_from_weekly[column], label=f'Daily Interpolated {column}', linestyle='-', marker='')\n",
    "\n",
    "# Plot für jede numerische Spalte in merged_df_weekly\n",
    "for column in merged_df_weekly.columns:\n",
    "    plt.plot(merged_df_weekly.index, merged_df_weekly[column], label=f'Weekly Data {column}', linestyle='--', marker='o')\n",
    "\n",
    "# Diagramm beschriften\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Weekly Data vs. Daily Interpolated Data')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "    # Diagramm anzeigen\n",
    "plt.show()\n",
    "    # Plot für jede numerische Spalte in daily_df_from_hourly\n",
    "\n",
    "#Diagramm für stündliche Daten\n",
    "plt.figure(figsize=(15, 6))\n",
    "for column in daily_from_hourly.columns:\n",
    "    plt.plot(daily_from_hourly.index, daily_from_hourly[column], label=f'Daily Interpolated {column}', linestyle='-', marker='')\n",
    "# Plot für jede numerische Spalte in merged_df_hourly\n",
    "for column in merged_df_hourly.columns:\n",
    "    plt.plot(merged_df_hourly.index, merged_df_hourly[column], label=f'Hourly Data {column}', linestyle='--', marker='o')\n",
    "\n",
    "# Legende, Achsentitel und Anzeige des Plots\n",
    "plt.xlabel(\"Datum\")\n",
    "plt.ylabel(\"Wert\")\n",
    "plt.title(\"Vergleich: Daily Interpolated vs. Hourly Data\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\"\n",
    "\n",
    "\"\"\"#alle monatlichen in getrennten plots plotten\n",
    "num_columns = len(daily_from_monthly.columns)  # Anzahl der Spalten\n",
    "fig, axes = plt.subplots(num_columns, 1, figsize=(15, 6 * num_columns))\n",
    "\n",
    "# Wenn es nur eine Spalte gibt, sorgt dieser Code dafür, dass axes eine einzelne Achse ist\n",
    "if num_columns == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Gehe jede Spalte durch und erstelle ein Diagramm\n",
    "for i, column in enumerate(daily_from_monthly.columns):\n",
    "    # Daily Interpolated plot\n",
    "    axes[i].plot(daily_from_monthly.index, daily_from_monthly[column], label=f'Daily Interpolated {column}', linestyle='-', marker='')\n",
    "\n",
    "    # Monthly Data plot\n",
    "    axes[i].plot(merged_df_monthly.index, merged_df_monthly[column], label=f'Monthly Data {column}', linestyle='--', marker='o')\n",
    "\n",
    "    # Achsenbeschriftungen und Titel\n",
    "    axes[i].set_xlabel(\"Datum\")\n",
    "    axes[i].set_ylabel(\"Wert\")\n",
    "    axes[i].set_title(f\"Vergleich: Daily Interpolated vs. Monthly Data ({column})\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True)\n",
    "\n",
    "# Layout anpassen, um Überschneidungen zu vermeiden\n",
    "plt.tight_layout()\n",
    "plt.show()\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Anomalies - Outliers and Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "- drop all weekend data\n",
    "- convert UnPlanned Unavailability into binary format\n",
    "- check whether data frame only hold numerical values\n",
    "- drop all columns that do not differ from others besides unit\n",
    "- insert new columns with sum of US export terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tage extrahieren\n",
    "fi_final_df['Day'] = fi_final_df.index.weekday+1\n",
    "\n",
    "fi_final_df = fi_final_df[~fi_final_df['Day'].isin([6,7])]\n",
    "fi_final_df = fi_final_df.drop(columns=['Day'])\n",
    "\n",
    "#Check whether the data frame only hold numerical values\n",
    "for column in fi_final_df:\n",
    "    if fi_final_df[column].dtype == 'object':\n",
    "        print(fi_final_df[column].name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handle missing values \n",
    "- delete all coumns with missing values > 40%\n",
    "- interpolate single misisng values linear\n",
    "- unify units "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Handling missing values\n",
    "# Alle missing values behandeln, die nicht interpoliert werden können\n",
    "fi_final_df = fi_final_df.loc[:, fi_final_df.isnull().mean() < 0.4] #Lösche Spalten mit missing values > 40%\n",
    "fi_final_df = fi_final_df.interpolate(method='linear')\n",
    "fi_final_df = fi_final_df.bfill()\n",
    "#Check whether there are missing values\n",
    "for col in fi_final_df.columns:\n",
    "    numberNA = fi_final_df[col].isna().sum()\n",
    "    if numberNA > 0:\n",
    "        print(f\"Column {col} has {numberNA} missing values.\")\n",
    "\n",
    "## Daten umrechnen & runden\n",
    "fi_final_df['A-Importiertes Erdgas Deutschland'] = fi_final_df['A-Importiertes Erdgas Deutschland'].mul(0.2778) # Terajoule in GWh\n",
    "fi_final_df['S-Power price'] = fi_final_df['S-Power price'].mul(10) # ct/KWh in €/MWh\n",
    "fi_final_df = fi_final_df.round(3)\n",
    "\n",
    "fi_y_final = fi_final_df[['S-Gas DA NL']]\n",
    "fi_final_df = fi_final_df.drop(columns=['S-Gas DA NL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Feature Selection \n",
    "columns_to_drop = [\n",
    "'N&P-Geschaeftslage (Deutschland)',\n",
    "    'N&P-Geschaeftserwartungen (Deutschland)',\n",
    "    'N&P-Geschaeftsklima (verarbeitendes Gewerbe)',\n",
    "    'N&P-Geschaeftslage (verarbeitendes Gewerbe)',\n",
    "    'N&P-Geschaeftserwartungen (verarbeitendes Gewerbe)',\n",
    "    'P-Germany_News_Index',\n",
    "    'A-Gas in storage (TWh)',\n",
    "    'A-gas storage Trend (%)',\n",
    "    'A-Stock/Cons (%)',\n",
    "    'A-Gas Injection (GWh/d)',\n",
    "    'N- Gas Withdrawal (GWh/d)',\n",
    "    'A-Norway Exit Nomination',\n",
    "    'A-Technical Capacity / DTMI (~GWh)',\n",
    "    'A-Send-out capacity (GWh/d)',\n",
    "    'K-Sunshine in s',\n",
    "    'P-EXY (BidNet)',\n",
    "    'S-Gas DA D',\n",
    "    'S-Gas DA UK',\n",
    "    'S-Gas DA FR',\n",
    "    'S-Gas DA AU',\n",
    "    'S-Gas DA IT',\n",
    "    'S-Coal Channel (Max)',\n",
    "    'S-Coal Channel (Min)',\n",
    "    'S-Coal Switching Price Average',\n",
    "    'A-US LNG Exportterminals',\n",
    "    'A-Flow Norway to Continent (excl UK)',\n",
    "    'A-Flow Norway to UK',\n",
    "    'A-Flow Russian Three Main Lines',\n",
    "    'A-Flow Russia to Bulgaria (TurkStream 2)',\n",
    "    'A-Flow Old Russian Routes to Poland (Drozdowicze, Wysokoje, Tietierowka & PWP)',\n",
    "    'A-Flow Russia to Hungary (Net VIP Bereg)',\n",
    "    'A-Flow North African Piped',\n",
    "    'A-Flow UK LNG Sendout',\n",
    "    'A-Flow Continental LNG Sendout', #wird irgendwie nicht gedropped??\n",
    "    'A-Flow Azerbaijan via TAP to Italy'\n",
    "]\n",
    "\n",
    "# Spalten entfernen\n",
    "#fi_final_df =fi_final_df.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export nach complete set, für Peak\n",
    "with open('fi_final_df.pkl', 'wb') as f:\n",
    "    pickle.dump(fi_final_df, f)\n",
    "with open('fi_y_final.pkl', 'wb') as f:\n",
    "    pickle.dump(fi_y_final, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvMasterarbeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
